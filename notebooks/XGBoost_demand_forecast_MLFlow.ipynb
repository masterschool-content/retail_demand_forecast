{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masterschool-content/retail_demand_forecast/blob/main/notebooks/XGBoost_demand_forecast_MLFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtHgPzRhqFWk"
      },
      "source": [
        "# Set-up MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b93uRLxJjJa",
        "outputId": "896abeec-6fc4-4e03-f2e5-326d8ff011b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.1/575.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install mlflow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "itH0RFsCqNWJ"
      },
      "outputs": [],
      "source": [
        "import mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnW8snKGsBP4"
      },
      "source": [
        "## Connect storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzWN0ScgOWRQ",
        "outputId": "70cddd8e-795c-453d-9b19-c0fa4d8ca6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BdydzLNTOYWC"
      },
      "outputs": [],
      "source": [
        "!mkdir mlflow_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i6JQPjs3Jl-S"
      },
      "outputs": [],
      "source": [
        "# To use MLFlow on Colab, we need to connect storage. We will use Google Drive\n",
        "\n",
        "# Define the MLflow storage path in Google Drive\n",
        "mlflow_storage_path = \"/content/drive/My Drive/mlflow_results\"\n",
        "\n",
        "# Set MLflow to log to the Google Drive directory\n",
        "mlflow.set_tracking_uri(f\"file:{mlflow_storage_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aja550T6qKgt",
        "outputId": "a560155c-e9eb-4fa3-a760-184b4c00e071"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///content/drive/My Drive/mlflow_results/810397914936930866', creation_time=1730832689901, experiment_id='810397914936930866', last_update_time=1730832689901, lifecycle_stage='active', name='Demand Forecast Experiment', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Set up experiment name\n",
        "mlflow.set_experiment(\"Demand Forecast Experiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khlw0Lx9sGQF"
      },
      "source": [
        "## Start MLFlow UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MrcjBTKoubiS"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0PIDzjK5uann"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjF4RMqZu4Lv",
        "outputId": "4e997645-bc3a-433b-f1d6-137658d2bad0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['mlflow', 'ui', '--backend-store-uri', '/con...>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", mlflow_storage_path])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "uZF2uAxrt3G-",
        "outputId": "a7a51b4d-938d-44a4-f080-5a3614d95bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2024-11-24T13:44:52+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-11-24T13:44:52+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-11-24T13:44:52+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2024-11-24T13:44:52+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-806ca9777932>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' * ngrok tunnel \\\"{public_url}\\\" '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    399\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "port=5000\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f' * ngrok tunnel \\\"{public_url}\\\" ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiBMsRj7Je00"
      },
      "source": [
        "# Import all libraries we will need for the modeling and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIBWNobU2zZ7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import joblib\n",
        "import mlflow.pyfunc\n",
        "import mlflow.models\n",
        "from mlflow.models.signature import infer_signature\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import xgboost as xgb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC3FT78H2nlt"
      },
      "source": [
        "# Read the files with pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: In the course we always load raw data and apply various transformation on it. To save time, you can create a notebook with data processing pipeline and store the output in final csv file. Later you simply read this preprocessed data in another notebook, where you work on the model. In this way you don't need to wait till data preprocessing is done every time you run the notebook to train the model and it will help you to ensure consistency if you train several models, without the need to copy-paste the code for data processing every time."
      ],
      "metadata": {
        "id": "pMckiQkN8t56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPlGo6nx2kR0"
      },
      "outputs": [],
      "source": [
        "# Load the CSV files into pandas DataFrames\n",
        "df_stores = pd.read_csv('/content/drive/MyDrive/retail_kaggle_data/stores.csv')\n",
        "df_items = pd.read_csv('/content/drive/MyDrive/retail_kaggle_data/items.csv')\n",
        "df_transactions = pd.read_csv('/content/drive/MyDrive/retail_kaggle_data/transactions.csv')\n",
        "df_oil = pd.read_csv('/content/drive/MyDrive/retail_kaggle_data/oil.csv')\n",
        "df_holidays_events = pd.read_csv('/content/drive/MyDrive/retail_kaggle_data/holidays_events.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the sales history for  the state 'Pichincha'"
      ],
      "metadata": {
        "id": "S9yi4M2XZwf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of store IDs for the state 'Guayas'\n",
        "store_ids = df_stores[df_stores['state'] == 'Pichincha']['store_nbr'].unique()\n",
        "\n",
        "# Initialize an empty list to hold filtered chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "# Define the chunk size (number of rows per chunk)\n",
        "chunk_size = 10 ** 6  # Adjust based on your system's memory capacity\n",
        "\n",
        "# Read the CSV file in chunks\n",
        "for chunk in pd.read_csv('/content/drive/MyDrive/retail_kaggle_data/train.csv', chunksize=chunk_size):\n",
        "    # Filter the chunk for the desired store IDs\n",
        "    chunk_filtered = chunk[chunk['store_nbr'].isin(store_ids)]\n",
        "    # Append the filtered chunk to the list\n",
        "    filtered_chunks.append(chunk_filtered)\n",
        "    # Optional: Delete the chunk to free up memory\n",
        "    del chunk\n",
        "\n",
        "# Concatenate all filtered chunks into a single DataFrame\n",
        "df_filtered = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "# Clean up to free memory\n",
        "del filtered_chunks"
      ],
      "metadata": {
        "id": "MKlpy_pJZ7Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For the quick experimenting, we will select just a few items"
      ],
      "metadata": {
        "id": "huu-_KDxaGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_id = [564533,838216,582865,364606]\n",
        "df_filtered = df_filtered[df_filtered['item_nbr'].isin(item_id)]\n",
        "\n",
        "# Group by date and aggregate sales\n",
        "df_filtered = df_filtered.groupby(['store_nbr','item_nbr','date']).sum()['unit_sales'].reset_index()"
      ],
      "metadata": {
        "id": "aJv5YO6vaJsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.shape"
      ],
      "metadata": {
        "id": "dDrI1GusaeMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToDnZcFJzgM4"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42dGb3Z00k5f"
      },
      "outputs": [],
      "source": [
        "data = df_filtered.copy() #due to historical reasons the dataframe is called \"data\" below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBAQfHEK-dy0"
      },
      "source": [
        "## Fill out missing dates with 0 sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw7u8Xn6-XEO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert 'date' column to datetime format\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "# Create an empty DataFrame to store the final result\n",
        "df_filled = pd.DataFrame()\n",
        "\n",
        "# Iterate through each store and item combination\n",
        "for (store, item), group in data.groupby(['store_nbr', 'item_nbr']):\n",
        "    # Get the minimum and maximum dates in the dataset to create a full date range\n",
        "    min_date = group['date'].min()\n",
        "    max_date = group['date'].max()\n",
        "    #print(min_date.date(), max_date.date())\n",
        "\n",
        "    # Create a full date range covering all days between the min and max dates\n",
        "    full_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "\n",
        "    # Set 'date' as index and sort by date\n",
        "    group.set_index('date', inplace=True)\n",
        "    group = group.sort_index()\n",
        "\n",
        "    # Reindex to fill missing dates with 0 sales\n",
        "    group = group.reindex(full_date_range, fill_value=0)\n",
        "\n",
        "    # Keep track of the store and item number for each row\n",
        "    group['store_nbr'] = store\n",
        "    group['item_nbr'] = item\n",
        "\n",
        "    # Ensure that missing sales values are filled with 0\n",
        "    group['unit_sales'] = group['unit_sales'].fillna(0)\n",
        "\n",
        "    # Append the group to the final DataFrame\n",
        "    df_filled = pd.concat([df_filled, group])\n",
        "\n",
        "# Reset the index to get 'date' back as a column\n",
        "df_filled.reset_index(inplace=True)\n",
        "df_filled.rename(columns={'index': 'date'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzYz6i8v-qWc"
      },
      "outputs": [],
      "source": [
        "data = df_filled.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dsckuuTyp1W"
      },
      "source": [
        "## Add more time-based features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vI_5r_zvWQj"
      },
      "outputs": [],
      "source": [
        "#data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQHFLO2b0Dow"
      },
      "outputs": [],
      "source": [
        "data['month'] = data['date'].dt.month\n",
        "data['day'] = data['date'].dt.day\n",
        "data['weekofyear'] = data['date'].dt.isocalendar().week\n",
        "data['dayofweek'] = data['date'].dt.dayofweek\n",
        "data['rolling_mean'] = data['unit_sales'].rolling(window=7).mean()\n",
        "data['rolling_std'] = data['unit_sales'].rolling(window=7).std()\n",
        "\n",
        "\n",
        "# Create lag features (e.g., sales from the previous day, previous week)\n",
        "data['lag_1'] = data['unit_sales'].shift(1)\n",
        "data['lag_7'] = data['unit_sales'].shift(7)\n",
        "data['lag_30'] = data['unit_sales'].shift(30)\n",
        "\n",
        "# Drop any rows with NaN values after creating lag features\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT-p8qkZyuU0"
      },
      "source": [
        "# Add features related to stores and items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3jE0nzDy1cz"
      },
      "outputs": [],
      "source": [
        "# Merge df_filtered with df_store and df_item on store_nbr and item_nbr, respectively\n",
        "data = data.merge(df_stores, on='store_nbr', how='left').merge(df_items, on='item_nbr', how='left')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Apply label encoding to each categorical column\n",
        "le = LabelEncoder()\n",
        "for col in ['city', 'state', 'type', 'family', 'class']:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "#categorical_columns = ['city', 'state', 'type', 'family', 'class']\n",
        "#for col in categorical_columns:\n",
        "#    data[col] = data[col].astype('category')\n",
        "data = data.sort_values(by=['store_nbr', 'item_nbr', 'date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYBMUmK468hL"
      },
      "outputs": [],
      "source": [
        "data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBapE3E3RBAz"
      },
      "source": [
        "# Split in test and train datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMli3QZ1REEU"
      },
      "outputs": [],
      "source": [
        "split_date = '2014-03-01'\n",
        "train = data[data['date'] < split_date]\n",
        "test = data[data['date'] >= split_date]\n",
        "print(\"Train dataframe shape:\",train.shape)\n",
        "print(\"Test dataframe shape:\",test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBUOm-5wB2Q6"
      },
      "source": [
        "# ML model with XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7WIWX-GCxcW"
      },
      "outputs": [],
      "source": [
        "# Define target variable (unit_sales) and features\n",
        "X_train = train.drop('unit_sales', axis=1)\n",
        "X_train = X_train.drop('date', axis=1)\n",
        "y_train = train['unit_sales']\n",
        "\n",
        "X_test = test.drop('unit_sales', axis=1)\n",
        "X_test = X_test.drop('date', axis=1)\n",
        "y_test = test['unit_sales']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzj8yGe05a6P"
      },
      "outputs": [],
      "source": [
        "# Initialize the XGBoost regressor\n",
        "xgboost_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=5, learning_rate=0.1, enable_categorical=True)\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1kX20_s5dDk"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics values and Plot the actual vs predicted values\n",
        "rmad_values = []\n",
        "bias_values = []\n",
        "rmse_values = []\n",
        "plot_paths = []\n",
        "plot_count  = 0\n",
        "for (item_nbr, store_nbr), group in data.groupby(['item_nbr', 'store_nbr']):\n",
        "  #group = group.groupby(['date']).sum()['unit_sales'].reset_index()\n",
        "  test_series = group[group['date'] >= split_date]\n",
        "  if(len(test_series)>10):\n",
        "    X_test = test_series.drop('unit_sales', axis=1)\n",
        "    X_test = X_test.drop('date', axis=1)\n",
        "    y_test = test_series['unit_sales']\n",
        "    # Make predictions on the test set\n",
        "    y_pred = xgboost_model.predict(X_test)\n",
        "\n",
        "    if plot_count<3:\n",
        "      train_series = group[group['date'] < split_date]\n",
        "      plt.figure(figsize=(8, 6))\n",
        "      plt.plot(train_series['date'], train_series['unit_sales'].values, label='Actual Sales', color='black')\n",
        "      plt.plot(test_series['date'], y_test.values, label='Actual Sales', color='blue')\n",
        "      plt.plot(test_series['date'], y_pred, label='Predicted Sales', color='red')\n",
        "      plt.title(f'Daily Sales of Item {item_nbr} at Store {store_nbr}')\n",
        "      file_path = f'ARIMA_forecast_store_{store_id}_item_{item_nbr}.png'\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "      plt.savefig(file_path)  # Saves the plot as a PNG file\n",
        "      plot_paths.append(file_path)\n",
        "      plot_count = plot_count + 1\n",
        "\n",
        "    #Calculate metric values for XGBoost model\n",
        "    # Bias calculation: mean of the residuals (y_pred - y_test)\n",
        "    bias = np.mean(y_pred - y_test)\n",
        "    bias_values.append(bias)\n",
        "    # Root Mean Absolute Deviation (RMAD)\n",
        "    rmad = np.mean(np.abs(y_pred - y_test))\n",
        "    rmad_values.append(rmad)\n",
        "    # Root Mean Squared Error (RMSE)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    rmse_values.append(rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OKojOqE9DZm"
      },
      "source": [
        "## Log metrics for XGBoost model in MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWB8g_qw9BLo"
      },
      "outputs": [],
      "source": [
        "import mlflow.xgboost\n",
        "with mlflow.start_run(run_name=\"XGBoost_store_item_features_run\"):\n",
        "    mlflow.log_param(\"objective\", \"reg:squarederror\") #value we used above\n",
        "    mlflow.log_param(\"n_estimators\", 100) #value we used above\n",
        "    mlflow.log_param(\"max_depth\", 5) #value we used above\n",
        "    mlflow.log_param(\"learning_rate\", 0.1) #value we used above\n",
        "    mlflow.log_metric(\"rMAD\", sum(rmad_values)/len(rmad_values)) #average rMAD\n",
        "    mlflow.log_metric(\"Bias\", sum(bias_values)/len(bias_values)) #average Bias\n",
        "    mlflow.log_metric(\"RMSE\", sum(rmse_values)/len(rmse_values)) #average RMSE\n",
        "    for plot_path in plot_paths:\n",
        "      mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n",
        "    # Log the XGBoost model\n",
        "    ## Example data (replace with a sample from your dataset)\n",
        "    #input_example = X_train[:5]\n",
        "    # Infer signature from input and output data\n",
        "    signature = infer_signature(X_train, xgboost_model.predict(X_train))\n",
        "    #print(signature)\n",
        "    #mlflow.sklearn.log_model(xgboost_model, \"xgboost_model\",\n",
        "    #                         signature=signature)\n",
        "    mlflow.xgboost.log_model(xgboost_model, \"xgboost_model\",\n",
        "                             signature=signature)\n",
        "    # Optionally log parameters, metrics, etc.\n",
        "    mlflow.log_params(xgboost_model.get_params())\n",
        "    mlflow.log_metric(\"train_score\", xgboost_model.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) save Pickle file with the model in Google Drive\n",
        "Next week we will create an application with our model. To serve model in the application, we create a pickle file below and next week we will use this file in our app.\n",
        "Note: you will experiment a lot and create many models. Pickle file you need only for the best model you create this week. To learn more about pickle files, please check the additional material suggested for this week."
      ],
      "metadata": {
        "id": "2-k9WMhWrpkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logged_model = 'runs:/288c9f141630411384c23e0a27a95d3f/xgboost_model' # you can find run id in MLFlow UI. See the video for more detailed instruction\n",
        "\n",
        "# Load model as a generic PyFuncModel.\n",
        "#loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
        "#input_schema = loaded_model.metadata.get_input_schema()\n",
        "#print(input_schema)\n",
        "\n",
        "# Load model as xgboost model\n",
        "loaded_model = mlflow.xgboost.load_model(logged_model)\n",
        "print(\"=== Model type\",type(loaded_model))"
      ],
      "metadata": {
        "id": "FGnGvO97sYq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the path to store pickle files of the models\n",
        "!mkdir /content/drive/MyDrive/models/"
      ],
      "metadata": {
        "id": "lkyOLUb25KXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the model as a pickle file in Google Drive\n",
        "pickle_path = '/content/drive/MyDrive/models/xgboost_model.pkl'  # Adjust Google Drive path as desired\n",
        "\n",
        "with open(pickle_path, 'wb') as f:\n",
        "    pickle.dump(loaded_model, f)\n",
        "\n",
        "print(f\"Model saved as pickle file at: {pickle_path}\")"
      ],
      "metadata": {
        "id": "_4qITcI_2a3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Z6RAHkEibO"
      },
      "source": [
        "# Next steps\n",
        "Now you can change the paramters of the model, fit the new model and compare the metrics in MLFlow. Also instead of manual tunning try Hyperparameter Tuning, does it give you better result then the default parameters?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}